{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "Train = pd.read_csv(\"Data/train.csv\")\n",
    "Test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd   \n",
    "from matplotlib import pyplot\n",
    "from numpy import mean\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.model_selection import train_test_split, RepeatedKFold, GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop('gender', axis=1, inplace=True)\n",
    "Test.drop('gender', axis=1, inplace=True)\n",
    "Train.drop('apache_3j_bodysystem', axis=1, inplace=True)\n",
    "Test.drop('apache_3j_bodysystem', axis=1, inplace=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#normalise Train['pre_icu_los_days']\n",
    "Train['pre_icu_los_days'] = np.log1p(Train['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "\n",
    "#normalise Train['pre_icu_los_days']\n",
    "Test['pre_icu_los_days'] = np.log1p(Test['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#simialrly normalise apache_2_diagnosis\n",
    "Train['apache_2_diagnosis'] = np.log1p(Train['apache_2_diagnosis'])\n",
    "Test['apache_2_diagnosis'] = np.log1p(Test['apache_2_diagnosis'])\n",
    "#do for apache_3j_diagnosis as well for both train and test\n",
    "Train['apache_3j_diagnosis'] = np.log1p(Train['apache_3j_diagnosis'])\n",
    "#do for test as well\n",
    "Test['apache_3j_diagnosis'] = np.log1p(Test['apache_3j_diagnosis'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for resprate_apache as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for temp_apache as well\n",
    "Train['temp_apache'] = np.log1p(Train['temp_apache'])\n",
    "#do for test as well\n",
    "Test['temp_apache'] = np.log1p(Test['temp_apache'])\n",
    "#do foor d1_resprate_max as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])\n",
    "#do for d1_temp_min\n",
    "Train['d1_temp_min'] = np.log1p(Train['d1_temp_min'])\n",
    "#do for test as well\n",
    "Test['d1_temp_min'] = np.log1p(Test['d1_temp_min'])\n",
    "#do foe h1_spo2_max\n",
    "Train['h1_spo2_max'] = np.log1p(Train['h1_spo2_max'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_max'] = np.log1p(Test['h1_spo2_max'])\n",
    "#do for h1_spo2_min\n",
    "Train['h1_spo2_min'] = np.log1p(Train['h1_spo2_min'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_min'] = np.log1p(Test['h1_spo2_min'])\n",
    "#do for d1_glucose_max\n",
    "Train['d1_glucose_max'] = np.log1p(Train['d1_glucose_max'])\n",
    "#do for test as well\n",
    "Test['d1_glucose_max'] = np.log1p(Test['d1_glucose_max'])\n",
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "# fill null values with median for temp_apache\n",
    "Train['temp_apache'].fillna(Train['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Train['d1_potassium_max'].fillna(Train['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Train['apache_4a_hospital_death_prob'].fillna(Train['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Train['apache_4a_icu_death_prob'].fillna(Train['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "Test['temp_apache'].fillna(Test['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Test['d1_potassium_max'].fillna(Test['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Test['apache_4a_hospital_death_prob'].fillna(Test['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Test['apache_4a_icu_death_prob'].fillna(Test['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Train.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Train['age'] = Train.apply(fill_age, axis=1)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Test.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Test['age'] = Test.apply(fill_age, axis=1)\n",
    "\n",
    "#for all binary columns we will apply mode imputation for missing values\n",
    "#first we will create a list of all binary columns\n",
    "binary_colsTest = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis']\n",
    "\n",
    "binary_colsTrain = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis','hospital_death']\n",
    "#now we will apply mode imputation on these columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "binary_colsTest = [col for col in Train.columns if Train[col].dtype == 'object' or col in binary_colsTest]\n",
    "binary_colsTrain = [col for col in Test.columns if Test[col].dtype == 'object' or col in binary_colsTrain]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "Train[binary_colsTrain] = imputer.fit_transform(Train[binary_colsTrain])\n",
    "Test[binary_colsTest] = imputer.fit_transform(Test[binary_colsTest])\n",
    "\n",
    "numeric_cols = [col for col in Train.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "numeric_colsTest = [col for col in Test.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create an instance of KNNImputer with k=5\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fill missing values in Train dataframe\n",
    "Train[numeric_cols] = imputer.fit_transform(Train[numeric_cols])\n",
    "\n",
    "# fill missing values in Test dataframe\n",
    "Test[numeric_colsTest] = imputer.fit_transform(Test[numeric_colsTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do one hot encoding\n",
    "Train = pd.get_dummies(Train)\n",
    "Test = pd.get_dummies(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x as all columns except target\n",
    "X=Train.loc[:,Train.columns!='hospital_death']\n",
    "y=Train['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainy, testy = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.850439 using {'max_depth': 7, 'min_samples_split': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# define the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# define the grid of hyperparameters to search\n",
    "grid = dict()\n",
    "grid['max_depth'] = [i for i in range(1, 11)]\n",
    "grid['min_samples_split'] = [i for i in range(2, 101, 2)]\n",
    "\n",
    "# define the evaluation procedure\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc')\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hamza\\Documents\\7th Sem\\IDM\\Challenge 1\\kfold.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/kfold.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(trainX, trainy)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39;49msleep(\u001b[39m0.01\u001b[39;49m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grid_search.fit(trainX, trainy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8496458326967409\n"
     ]
    }
   ],
   "source": [
    "#now i want to use the best parameters to train my model\n",
    "model = DecisionTreeClassifier(max_depth=7, min_samples_split=100)\n",
    "model.fit(trainX, trainy)\n",
    "#check accurcy using roc\n",
    "yhat = model.predict_proba(testX)\n",
    "# keep probabilities for the positive outcome only\n",
    "yhat = yhat[:, 1]\n",
    "# calculate roc curves\n",
    "print(roc_auc_score(testy, yhat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_death_prob = model.predict_proba(Test)\n",
    "# The predicted probabilities of each class are stored in the second column of the output array\n",
    "hospital_death = hospital_death_prob[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionDT.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionKFOLD.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
