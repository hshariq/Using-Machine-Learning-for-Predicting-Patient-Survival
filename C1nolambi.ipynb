{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "Train = pd.read_csv(\"Data/train.csv\")\n",
    "Test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill null values with median for temp_apache\n",
    "Train['temp_apache'].fillna(Train['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Train['d1_potassium_max'].fillna(Train['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Train['apache_4a_hospital_death_prob'].fillna(Train['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Train['apache_4a_icu_death_prob'].fillna(Train['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "Test['temp_apache'].fillna(Test['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Test['d1_potassium_max'].fillna(Test['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Test['apache_4a_hospital_death_prob'].fillna(Test['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Test['apache_4a_icu_death_prob'].fillna(Test['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Train.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Train['age'] = Train.apply(fill_age, axis=1)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Test.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Test['age'] = Test.apply(fill_age, axis=1)\n",
    "\n",
    "#for all binary columns we will apply mode imputation for missing values\n",
    "#first we will create a list of all binary columns\n",
    "binary_colsTest = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis']\n",
    "\n",
    "binary_colsTrain = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis','hospital_death']\n",
    "#now we will apply mode imputation on these columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "binary_colsTest = [col for col in Train.columns if Train[col].dtype == 'object' or col in binary_colsTest]\n",
    "binary_colsTrain = [col for col in Test.columns if Test[col].dtype == 'object' or col in binary_colsTrain]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "Train[binary_colsTrain] = imputer.fit_transform(Train[binary_colsTrain])\n",
    "Test[binary_colsTest] = imputer.fit_transform(Test[binary_colsTest])\n",
    "\n",
    "numeric_cols = [col for col in Train.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "numeric_colsTest = [col for col in Test.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create an instance of KNNImputer with k=3\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# fill missing values in Train dataframe\n",
    "Train[numeric_cols] = imputer.fit_transform(Train[numeric_cols])\n",
    "\n",
    "# fill missing values in Test dataframe\n",
    "Test[numeric_colsTest] = imputer.fit_transform(Test[numeric_colsTest])\n",
    "\n",
    "dropcolumns=['d1_diasbp_noninvasive_min','h1_sysbp_max','h1_mbp_max', 'h1_mbp_noninvasive_max', 'h1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min', 'h1_diasbp_noninvasive_min']\n",
    "Train= Train.drop(dropcolumns, axis=1)\n",
    "Test= Test.drop(dropcolumns, axis=1)\n",
    "\n",
    "#next we drop simialr record columns\n",
    "Train = Train.drop('apache_3j_bodysystem', axis=1)\n",
    "Test = Test.drop('apache_3j_bodysystem', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode Train and Test\n",
    "Traindum = pd.get_dummies(Train)\n",
    "Testdum = pd.get_dummies(Test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale using zscore standerdization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "Traindum = scaler.fit_transform(Traindum)\n",
    "Testdum = scaler.fit_transform(Testdum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hamza\\Documents\\7th Sem\\IDM\\Challenge 1\\C1nolambi.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m scaler \u001b[39m=\u001b[39m MinMaxScaler()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m Train_scaled \u001b[39m=\u001b[39m scaler\u001b[39m.\u001b[39mfit_transform(Train)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m Train_scaled \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(Train_scaled, columns\u001b[39m=\u001b[39mTrain\u001b[39m.\u001b[39;49mcolumns)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "Train_scaled = scaler.fit_transform(Train)\n",
    "Train_scaled = pd.DataFrame(Train_scaled, columns=Train.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'hospital_death'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hamza\\Documents\\7th Sem\\IDM\\Challenge 1\\C1nolambi.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m X \u001b[39m=\u001b[39m Train\u001b[39m.\u001b[39mloc[: , Train\u001b[39m.\u001b[39mcolumns \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mhospital_death\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m y \u001b[39m=\u001b[39m Train[\u001b[39m'\u001b[39;49m\u001b[39mhospital_death\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/C1nolambi.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m X_train, X_val, y_train, y_val \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m, random_state\u001b[39m=\u001b[39m\u001b[39m42\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[0;32m   3808\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexes\\range.py:395\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[1;32m--> 395\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[0;32m    396\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39mget_loc(key, method\u001b[39m=\u001b[39mmethod, tolerance\u001b[39m=\u001b[39mtolerance)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'hospital_death'"
     ]
    }
   ],
   "source": [
    "X = Train.loc[: , Train.columns != 'hospital_death']\n",
    "y = Train['hospital_death']\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record the start time\n",
    "nb_c = CatBoostClassifier(iterations=650, depth=3, learning_rate=0.1, loss_function='Logloss', verbose=False)\n",
    "nb_c.fit(X_train,y_train)\n",
    "md_probs = nb_c.predict_proba(X_val)\n",
    "md_probs = md_probs[:,1]\n",
    "md_auc = roc_auc_score(y_val, md_probs)\n",
    "print(\"Cat Boost\" , \" : \", md_auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
