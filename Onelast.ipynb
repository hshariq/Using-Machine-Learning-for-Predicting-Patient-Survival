{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n",
    "Train = pd.read_csv(\"Data/train.csv\")\n",
    "Test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop('gender', axis=1, inplace=True)\n",
    "Test.drop('gender', axis=1, inplace=True)\n",
    "Train.drop('apache_3j_bodysystem', axis=1, inplace=True)\n",
    "Test.drop('apache_3j_bodysystem', axis=1, inplace=True)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#normalise Train['pre_icu_los_days']\n",
    "Train['pre_icu_los_days'] = np.log1p(Train['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "\n",
    "#normalise Train['pre_icu_los_days']\n",
    "Test['pre_icu_los_days'] = np.log1p(Test['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#simialrly normalise apache_2_diagnosis\n",
    "Train['apache_2_diagnosis'] = np.log1p(Train['apache_2_diagnosis'])\n",
    "Test['apache_2_diagnosis'] = np.log1p(Test['apache_2_diagnosis'])\n",
    "#do for apache_3j_diagnosis as well for both train and test\n",
    "Train['apache_3j_diagnosis'] = np.log1p(Train['apache_3j_diagnosis'])\n",
    "#do for test as well\n",
    "Test['apache_3j_diagnosis'] = np.log1p(Test['apache_3j_diagnosis'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for resprate_apache as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for temp_apache as well\n",
    "Train['temp_apache'] = np.log1p(Train['temp_apache'])\n",
    "#do for test as well\n",
    "Test['temp_apache'] = np.log1p(Test['temp_apache'])\n",
    "#do foor d1_resprate_max as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])\n",
    "#do for d1_temp_min\n",
    "Train['d1_temp_min'] = np.log1p(Train['d1_temp_min'])\n",
    "#do for test as well\n",
    "Test['d1_temp_min'] = np.log1p(Test['d1_temp_min'])\n",
    "#do foe h1_spo2_max\n",
    "Train['h1_spo2_max'] = np.log1p(Train['h1_spo2_max'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_max'] = np.log1p(Test['h1_spo2_max'])\n",
    "#do for h1_spo2_min\n",
    "Train['h1_spo2_min'] = np.log1p(Train['h1_spo2_min'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_min'] = np.log1p(Test['h1_spo2_min'])\n",
    "#do for d1_glucose_max\n",
    "Train['d1_glucose_max'] = np.log1p(Train['d1_glucose_max'])\n",
    "#do for test as well\n",
    "Test['d1_glucose_max'] = np.log1p(Test['d1_glucose_max'])\n",
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "# fill null values with median for temp_apache\n",
    "Train['temp_apache'].fillna(Train['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Train['d1_potassium_max'].fillna(Train['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Train['apache_4a_hospital_death_prob'].fillna(Train['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Train['apache_4a_icu_death_prob'].fillna(Train['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "Test['temp_apache'].fillna(Test['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Test['d1_potassium_max'].fillna(Test['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Test['apache_4a_hospital_death_prob'].fillna(Test['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Test['apache_4a_icu_death_prob'].fillna(Test['apache_4a_icu_death_prob'].median(), inplace=True)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Train.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Train['age'] = Train.apply(fill_age, axis=1)\n",
    "\n",
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Test.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Test['age'] = Test.apply(fill_age, axis=1)\n",
    "\n",
    "#for all binary columns we will apply mode imputation for missing values\n",
    "#first we will create a list of all binary columns\n",
    "binary_colsTest = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis']\n",
    "\n",
    "binary_colsTrain = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis','hospital_death']\n",
    "#now we will apply mode imputation on these columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "binary_colsTest = [col for col in Train.columns if Train[col].dtype == 'object' or col in binary_colsTest]\n",
    "binary_colsTrain = [col for col in Test.columns if Test[col].dtype == 'object' or col in binary_colsTrain]\n",
    "\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "Train[binary_colsTrain] = imputer.fit_transform(Train[binary_colsTrain])\n",
    "Test[binary_colsTest] = imputer.fit_transform(Test[binary_colsTest])\n",
    "\n",
    "numeric_cols = [col for col in Train.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "numeric_colsTest = [col for col in Test.select_dtypes(include=[np.number]).columns if col not in binary_colsTrain]\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create an instance of KNNImputer with k=5\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fill missing values in Train dataframe\n",
    "Train[numeric_cols] = imputer.fit_transform(Train[numeric_cols])\n",
    "\n",
    "# fill missing values in Test dataframe\n",
    "Test[numeric_colsTest] = imputer.fit_transform(Test[numeric_colsTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:2: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Train = pd.get_dummies(Train)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n",
      "C:\\Users\\hamza\\AppData\\Local\\Temp\\ipykernel_11524\\1147109646.py:3: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  Test = pd.get_dummies(Test)\n"
     ]
    }
   ],
   "source": [
    "#one hot encode\n",
    "Train = pd.get_dummies(Train)\n",
    "Test = pd.get_dummies(Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = [col for col in Train.columns if Train[col].dtype == 'object']\n",
    "caTest = [col for col in Test.columns if Test[col].dtype == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in cat:\n",
    "    Train[col] = label_encoder.fit_transform(Train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#label encode\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in caTest:\n",
    "    Test[col] = label_encoder.fit_transform(Test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = Train.loc[:, Train.columns != 'hospital_death']\n",
    "y = Train['hospital_death']\n",
    "\n",
    "trainXd, testXd, trainyd, testyd = train_test_split(X, y, test_size=0.3, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.78062238e-03, 5.21453487e-03, 1.00178043e-02, 2.78390653e-03,\n",
       "       0.00000000e+00, 5.95749110e-04, 4.71824580e-03, 0.00000000e+00,\n",
       "       7.05225521e-03, 0.00000000e+00, 5.59384123e-03, 5.24497699e-03,\n",
       "       1.18604279e-02, 0.00000000e+00, 3.83352331e-03, 2.72858250e-03,\n",
       "       0.00000000e+00, 3.11004552e-03, 7.69914311e-03, 0.00000000e+00,\n",
       "       2.37644975e-03, 1.37532558e-03, 5.95694662e-03, 2.68471200e-02,\n",
       "       1.18176061e-02, 9.20887177e-03, 1.48625569e-03, 1.33804223e-02,\n",
       "       1.14576249e-02, 6.27042593e-02, 1.40290661e-02, 0.00000000e+00,\n",
       "       1.47927045e-02, 4.28404356e-03, 0.00000000e+00, 0.00000000e+00,\n",
       "       0.00000000e+00, 0.00000000e+00, 5.74805365e-03, 4.31547900e-03,\n",
       "       0.00000000e+00, 2.45127351e-03, 1.11012676e-02, 6.68965153e-03,\n",
       "       0.00000000e+00, 3.08783439e-04, 0.00000000e+00, 9.58175248e-04,\n",
       "       2.64221626e-03, 1.14415632e-02, 1.50198023e-02, 1.83393493e-01,\n",
       "       5.01979886e-01, 0.00000000e+00, 0.00000000e+00])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(max_depth=7)  \n",
    "model.fit(trainXd,trainyd)\n",
    "model.feature_importances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp = pd.Series(model.feature_importances_,index=trainXd.columns).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract column names whose importance is less than 0.01 # 0.002\n",
    "col_to_drop = feature_imp[feature_imp < 0.01].index\n",
    "#remove these columns from the training and test sets\n",
    "trainXDROP = trainXd.drop(col_to_drop, axis=1)\n",
    "testXDROP = testXd.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDrop=Test.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8548260021501034\n",
      "0.8209288143721543\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier as DecisionTre\n",
    "from sklearn.naive_bayes import GaussianNB as GNB\n",
    "# Transform X_test to contain only the selected features\n",
    "DT = DecisionTre(max_depth=7, min_samples_leaf=60)\n",
    "DT.fit(trainXDROP, trainyd)\n",
    "\n",
    "GN=GNB()\n",
    "GN.fit(trainXDROP, trainyd)\n",
    "\n",
    "# Predict and evaluate on the transformed X_test\n",
    "y_pred = DT.predict_proba(testXDROP)[:, 1]\n",
    "# y_pred2= kn.predict_proba(Testtransofrm)[:, 1]\n",
    "print(roc_auc_score(testyd, y_pred))\n",
    "\n",
    "# Predict and evaluate on the transformed X_test\n",
    "y_pred2 = GN.predict_proba(testXDROP)[:, 1]\n",
    "# y_pred2= kn.predict_proba(Testtransofrm)[:, 1]\n",
    "print(roc_auc_score(testyd, y_pred2))\n",
    "# print(roc_auc_score(testy, y_pred2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_death_prob = DT.predict_proba(testDrop)\n",
    "# The predicted probabilities of each class are stored in the second column of the output array\n",
    "hospital_death = hospital_death_prob[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionsfeatureimp.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FEATURE SELECTOR ON DECSION TREE    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.23.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.3.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (3.6.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.24.2->mlxtend) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x as all columns except target\n",
    "X=Train.loc[:,Train.columns!='hospital_death']\n",
    "y=Train['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run categoricalNB on categorical_cols \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets with a test size of 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   16.8s\n",
      "\n",
      "[2023-09-29 20:05:47] Features: 1/50 -- score: 0.8422963499503415[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   21.5s\n",
      "\n",
      "[2023-09-29 20:06:21] Features: 2/50 -- score: 0.8479467955141571[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   30.1s\n",
      "\n",
      "[2023-09-29 20:07:05] Features: 3/50 -- score: 0.8479467955141571[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   32.4s\n",
      "\n",
      "[2023-09-29 20:07:54] Features: 4/50 -- score: 0.8471702330121454[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   32.9s\n",
      "\n",
      "[2023-09-29 20:08:44] Features: 5/50 -- score: 0.845911021537925[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   31.8s\n",
      "\n",
      "[2023-09-29 20:09:30] Features: 6/50 -- score: 0.8451728890639603[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   32.8s\n",
      "\n",
      "[2023-09-29 20:10:18] Features: 7/50 -- score: 0.8451728890639603[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   40.8s\n",
      "\n",
      "[2023-09-29 20:11:15] Features: 8/50 -- score: 0.8436811177801639[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   31.6s\n",
      "\n",
      "[2023-09-29 20:12:03] Features: 9/50 -- score: 0.8420379135454384[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   36.7s\n",
      "\n",
      "[2023-09-29 20:13:02] Features: 10/50 -- score: 0.8397617053602501[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   44.0s\n",
      "\n",
      "[2023-09-29 20:14:04] Features: 11/50 -- score: 0.8374782463091714[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   39.0s\n",
      "\n",
      "[2023-09-29 20:15:03] Features: 12/50 -- score: 0.8346468667046836[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   41.8s\n",
      "\n",
      "[2023-09-29 20:16:06] Features: 13/50 -- score: 0.8307710773345693[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   44.0s\n",
      "\n",
      "[2023-09-29 20:17:11] Features: 14/50 -- score: 0.8248314663617423[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   46.8s\n",
      "\n",
      "[2023-09-29 20:18:21] Features: 15/50 -- score: 0.8181162228008908[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   47.5s\n",
      "\n",
      "[2023-09-29 20:19:32] Features: 16/50 -- score: 0.8109319793033241[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   47.4s\n",
      "\n",
      "[2023-09-29 20:20:43] Features: 17/50 -- score: 0.8109699375938613[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   48.1s\n",
      "\n",
      "[2023-09-29 20:21:53] Features: 18/50 -- score: 0.8028450303081845[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   53.5s\n",
      "\n",
      "[2023-09-29 20:23:10] Features: 19/50 -- score: 0.8019443392298005[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   58.9s\n",
      "\n",
      "[2023-09-29 20:24:36] Features: 20/50 -- score: 0.7961415556126933[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   56.4s\n",
      "\n",
      "[2023-09-29 20:25:58] Features: 21/50 -- score: 0.7879585804521463[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   57.2s\n",
      "\n",
      "[2023-09-29 20:27:23] Features: 22/50 -- score: 0.7808415267433858[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   58.7s\n",
      "\n",
      "[2023-09-29 20:28:48] Features: 23/50 -- score: 0.7800099006055395[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.0min\n",
      "\n",
      "[2023-09-29 20:30:18] Features: 24/50 -- score: 0.7728972327553641[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.0min\n",
      "\n",
      "[2023-09-29 20:31:46] Features: 25/50 -- score: 0.7716695731968529[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.1min\n",
      "\n",
      "[2023-09-29 20:33:19] Features: 26/50 -- score: 0.7695694756643582[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.2min\n",
      "\n",
      "[2023-09-29 20:34:59] Features: 27/50 -- score: 0.7624377964691333[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:36:47] Features: 28/50 -- score: 0.7555837061932433[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.2min\n",
      "\n",
      "[2023-09-29 20:38:30] Features: 29/50 -- score: 0.755412666471549[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:40:13] Features: 30/50 -- score: 0.7492167977053247[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:41:57] Features: 31/50 -- score: 0.7408184477884262[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:43:40] Features: 32/50 -- score: 0.7286130534714734[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:45:21] Features: 33/50 -- score: 0.7191784412704029[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:47:01] Features: 34/50 -- score: 0.7102644223824077[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-29 20:48:44] Features: 35/50 -- score: 0.6986460996496195[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.4min\n",
      "\n",
      "[2023-09-29 20:50:30] Features: 36/50 -- score: 0.6873445106763182[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.4min\n",
      "\n",
      "[2023-09-29 20:52:18] Features: 37/50 -- score: 0.6785555563774585[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n",
      "\n",
      "[2023-09-29 20:54:12] Features: 38/50 -- score: 0.6741614580621297[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n",
      "\n",
      "[2023-09-29 20:56:07] Features: 39/50 -- score: 0.6684117804008445[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.6min\n",
      "\n",
      "[2023-09-29 20:57:59] Features: 40/50 -- score: 0.6649764999072945[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n",
      "\n",
      "[2023-09-29 20:59:49] Features: 41/50 -- score: 0.6569794664920502[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.6min\n",
      "\n",
      "[2023-09-29 21:01:40] Features: 42/50 -- score: 0.6543114693740153[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.6min\n",
      "\n",
      "[2023-09-29 21:03:33] Features: 43/50 -- score: 0.6483538128890854[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.7min\n",
      "\n",
      "[2023-09-29 21:05:27] Features: 44/50 -- score: 0.6441708753756904[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.7min\n",
      "\n",
      "[2023-09-29 21:07:21] Features: 45/50 -- score: 0.6448159131327336[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.8min\n",
      "\n",
      "[2023-09-29 21:09:16] Features: 46/50 -- score: 0.6327097298047233[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.9min\n",
      "\n",
      "[2023-09-29 21:11:18] Features: 47/50 -- score: 0.6338385354183587[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  2.1min\n",
      "\n",
      "[2023-09-29 21:13:30] Features: 48/50 -- score: 0.6361840140234966[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  2.6min\n",
      "\n",
      "[2023-09-29 21:16:09] Features: 49/50 -- score: 0.6376197525589016[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  2.7min\n",
      "\n",
      "[2023-09-29 21:18:52] Features: 50/50 -- score: 0.6347003116388676"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "fwd_feature_selector = SFS(DecisionTreeClassifier(), k_features=(20,50), forward=True,\n",
    "                                                  floating=False, verbose=2, scoring='roc_auc', cv=5).fit(X_train, y_train)\n",
    "#cv is croos validation 5 times\n",
    "#The cv=5 parameter means that this process is done using 5-fold cross-validation on your training data (X_train and y_train). \n",
    "# In each fold of the cross-validation, 4/5 of the data is used for training and 1/5 is used for validation.\n",
    "# The ROC AUC score is averaged over the 5 folds to give a more robust estimate of the modelâ€™s performance.\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fwd_feature_selector.k_feature_names_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844419340913708\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Transform X_test to contain only the selected features\n",
    "X_test_transformed = fwd_feature_selector.transform(X_test)\n",
    "\n",
    "# Fit the model on the transformed X_train\n",
    "pipe_kn = Pipeline([(\"scaler\", MinMaxScaler()), (\"knr\", DecisionTre(max_depth=5,min_samples_split=500))])\n",
    "# model = DecisionTreeClassifier(max_depth=10,min_samples_split=1100)\n",
    "pipe_kn.fit(fwd_feature_selector.transform(X_train), y_train)\n",
    "\n",
    "# Predict and evaluate on the transformed X_test\n",
    "y_pred = pipe_kn.predict_proba(X_test_transformed)[:, 1]\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier as KNN\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('feature_selector', SFS(DecisionTreeClassifier(), k_features=(20,50), forward=True,\n",
    "                             floating=False, verbose=2, scoring='roc_auc', cv=5)),\n",
    "    ('classifier', KNN(n_neighbors=500))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate on the test data\n",
    "y_pred = pipeline.predict_proba(X_test)[:, 1]\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospital_death_prob = model.predict_proba(fwd_feature_selector.transform(Test))\n",
    "# The predicted probabilities of each class are stored in the second column of the output array\n",
    "hospital_death = hospital_death_prob[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionsFWDSLEECTION.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
