{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from io import StringIO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train = pd.read_csv(\"Data/train.csv\")\n",
    "Test = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train.drop('gender', axis=1, inplace=True)\n",
    "Test.drop('gender', axis=1, inplace=True)\n",
    "Train.drop('apache_3j_bodysystem', axis=1, inplace=True)\n",
    "Test.drop('apache_3j_bodysystem', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#normalise Train['pre_icu_los_days']\n",
    "Train['pre_icu_los_days'] = np.log1p(Train['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "plt.hist(Train['pre_icu_los_days'], bins=20, alpha=0.5, label='Pre ICU LOS Days')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalise Train['pre_icu_los_days']\n",
    "Test['pre_icu_los_days'] = np.log1p(Test['pre_icu_los_days'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "plt.hist(Test['pre_icu_los_days'], bins=20, alpha=0.5, label='Pre ICU LOS Days')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simialrly normalise apache_2_diagnosis\n",
    "Train['apache_2_diagnosis'] = np.log1p(Train['apache_2_diagnosis'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "plt.hist(Train['apache_2_diagnosis'], bins=20, alpha=0.5, label='apache_2_diagnosis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simialrly normalise apache_2_diagnosis\n",
    "Test['apache_2_diagnosis'] = np.log1p(Test['apache_2_diagnosis'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for apache_3j_diagnosis as well for both train and test\n",
    "Train['apache_3j_diagnosis'] = np.log1p(Train['apache_3j_diagnosis'])\n",
    "#do for test as well\n",
    "Test['apache_3j_diagnosis'] = np.log1p(Test['apache_3j_diagnosis'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for resprate_apache as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])\n",
    "#make hsitogram of normlaised Train['pre_icu_los_days']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for temp_apache as well\n",
    "Train['temp_apache'] = np.log1p(Train['temp_apache'])\n",
    "#do for test as well\n",
    "Test['temp_apache'] = np.log1p(Test['temp_apache'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do foor d1_resprate_max as well\n",
    "Train['d1_resprate_max'] = np.log1p(Train['d1_resprate_max'])\n",
    "#do for test as well\n",
    "Test['d1_resprate_max'] = np.log1p(Test['d1_resprate_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for d1_temp_min\n",
    "Train['d1_temp_min'] = np.log1p(Train['d1_temp_min'])\n",
    "#do for test as well\n",
    "Test['d1_temp_min'] = np.log1p(Test['d1_temp_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do foe h1_spo2_max\n",
    "Train['h1_spo2_max'] = np.log1p(Train['h1_spo2_max'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_max'] = np.log1p(Test['h1_spo2_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for h1_spo2_min\n",
    "Train['h1_spo2_min'] = np.log1p(Train['h1_spo2_min'])\n",
    "#do for test as well\n",
    "Test['h1_spo2_min'] = np.log1p(Test['h1_spo2_min'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do for d1_glucose_max\n",
    "Train['d1_glucose_max'] = np.log1p(Train['d1_glucose_max'])\n",
    "#do for test as well\n",
    "Test['d1_glucose_max'] = np.log1p(Test['d1_glucose_max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "# fill null values with median for temp_apache\n",
    "Train['temp_apache'].fillna(Train['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Train['d1_potassium_max'].fillna(Train['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Train['apache_4a_hospital_death_prob'].fillna(Train['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Train['apache_4a_icu_death_prob'].fillna(Train['apache_4a_icu_death_prob'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#since all these r heavily skewed andaffected by the outliers we will fill them using median imputation\n",
    "\n",
    "Test['temp_apache'].fillna(Test['temp_apache'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for d1_potassium_max\n",
    "Test['d1_potassium_max'].fillna(Test['d1_potassium_max'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_hospital_death_prob\n",
    "Test['apache_4a_hospital_death_prob'].fillna(Test['apache_4a_hospital_death_prob'].median(), inplace=True)\n",
    "\n",
    "# fill null values with median for apache_4a_icu_death_prob\n",
    "Test['apache_4a_icu_death_prob'].fillna(Test['apache_4a_icu_death_prob'].median(), inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Train.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Train['age'] = Train.apply(fill_age, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the dataframe by apache_2_bodysystem and calculate the mean age for each group\n",
    "mean_age_by_bodysystem = Test.groupby('apache_2_bodysystem')['age'].mean()\n",
    "\n",
    "# define a function that takes a row of the dataframe as input and returns the mean age of the corresponding apache_2_bodysystem\n",
    "def fill_age(row):\n",
    "    if pd.isnull(row['age']):\n",
    "        if pd.isnull(row['apache_2_bodysystem']):\n",
    "            return np.nan\n",
    "        else:\n",
    "            return mean_age_by_bodysystem[row['apache_2_bodysystem']]\n",
    "    else:\n",
    "        return row['age']\n",
    "\n",
    "# apply the function to each row of the dataframe and fill the missing age values with the corresponding mean age\n",
    "Test['age'] = Test.apply(fill_age, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all binary columns we will apply mode imputation for missing values\n",
    "#first we will create a list of all binary columns\n",
    "binary_cols = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis']\n",
    "#now we will apply mode imputation on these columns\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "Train[binary_cols] = imputer.fit_transform(Train[binary_cols])\n",
    "Test[binary_cols] = imputer.fit_transform(Test[binary_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_cols = ['elective_surgery', 'apache_post_operative', 'gcs_unable_apache', 'intubated_apache', 'ventilated_apache','immunosuppression', 'solid_tumor_with_metastasis']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [col for col in Train.columns if Train[col].dtype == 'object' or col in binary_cols]\n",
    "categorical_colsTest = [col for col in Test.columns if Test[col].dtype == 'object' or col in binary_cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = [col for col in Train.select_dtypes(include=[np.number]).columns if col not in binary_cols]\n",
    "numeric_colsTest = [col for col in Test.select_dtypes(include=[np.number]).columns if col not in binary_cols]\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# create an instance of KNNImputer with k=5\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "\n",
    "# fill missing values in Train dataframe\n",
    "Train[numeric_cols] = imputer.fit_transform(Train[numeric_cols])\n",
    "\n",
    "# fill missing values in Test dataframe\n",
    "Test[numeric_colsTest] = imputer.fit_transform(Test[numeric_colsTest])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RecordID                         0\n",
       "hospital_id                      0\n",
       "icu_id                           0\n",
       "ethnicity                        0\n",
       "icu_admit_source                 0\n",
       "icu_stay_type                    0\n",
       "icu_type                         0\n",
       "apache_2_bodysystem              0\n",
       "age                              0\n",
       "elective_surgery                 0\n",
       "pre_icu_los_days                 0\n",
       "apache_2_diagnosis               0\n",
       "apache_3j_diagnosis              0\n",
       "apache_post_operative            0\n",
       "gcs_eyes_apache                  0\n",
       "gcs_motor_apache                 0\n",
       "gcs_unable_apache                0\n",
       "gcs_verbal_apache                0\n",
       "heart_rate_apache                0\n",
       "intubated_apache                 0\n",
       "resprate_apache                  0\n",
       "temp_apache                      0\n",
       "ventilated_apache                0\n",
       "d1_diasbp_min                    0\n",
       "d1_diasbp_noninvasive_min        0\n",
       "d1_heartrate_max                 0\n",
       "d1_mbp_min                       0\n",
       "d1_mbp_noninvasive_min           0\n",
       "d1_resprate_max                  0\n",
       "d1_spo2_min                      0\n",
       "d1_sysbp_min                     0\n",
       "d1_sysbp_noninvasive_min         0\n",
       "d1_temp_min                      0\n",
       "h1_diasbp_min                    0\n",
       "h1_diasbp_noninvasive_min        0\n",
       "h1_heartrate_max                 0\n",
       "h1_heartrate_min                 0\n",
       "h1_mbp_max                       0\n",
       "h1_mbp_min                       0\n",
       "h1_mbp_noninvasive_max           0\n",
       "h1_mbp_noninvasive_min           0\n",
       "h1_resprate_max                  0\n",
       "h1_resprate_min                  0\n",
       "h1_spo2_max                      0\n",
       "h1_spo2_min                      0\n",
       "h1_sysbp_max                     0\n",
       "h1_sysbp_min                     0\n",
       "h1_sysbp_noninvasive_max         0\n",
       "h1_sysbp_noninvasive_min         0\n",
       "d1_glucose_max                   0\n",
       "d1_potassium_max                 0\n",
       "apache_4a_hospital_death_prob    0\n",
       "apache_4a_icu_death_prob         0\n",
       "immunosuppression                0\n",
       "solid_tumor_with_metastasis      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mlxtend in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (0.23.0)\n",
      "Requirement already satisfied: scipy>=1.2.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.9.3)\n",
      "Requirement already satisfied: numpy>=1.16.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.23.5)\n",
      "Requirement already satisfied: pandas>=0.24.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.5.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.3.1)\n",
      "Requirement already satisfied: matplotlib>=3.0.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (3.6.3)\n",
      "Requirement already satisfied: joblib>=0.13.2 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from mlxtend) (1.3.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (4.42.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from matplotlib>=3.0.0->mlxtend) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from pandas>=0.24.2->mlxtend) (2022.7.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from scikit-learn>=1.0.2->mlxtend) (3.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\hamza\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages (from python-dateutil>=2.7->matplotlib>=3.0.0->mlxtend) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "%pip install mlxtend\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 55)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do label encoding for train and test\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#i want to label encoding on all my cateogircal data\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    Train[col] = le.fit_transform(Train[col])\n",
    "    Test[col] = le.fit_transform(Test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define x as all columns except target\n",
    "X=Train.loc[:,Train.columns!='hospital_death']\n",
    "y=Train['hospital_death']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now run categoricalNB on categorical_cols \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into train and test sets with a test size of 0.3\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   12.2s\n",
      "\n",
      "[2023-09-27 13:30:44] Features: 1/40 -- score: 0.8422123121803624[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   16.5s\n",
      "\n",
      "[2023-09-27 13:31:06] Features: 2/40 -- score: 0.8457698781634481[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   19.5s\n",
      "\n",
      "[2023-09-27 13:31:32] Features: 3/40 -- score: 0.8430944965357501[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   20.2s\n",
      "\n",
      "[2023-09-27 13:31:58] Features: 4/40 -- score: 0.8333875578385648[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   22.1s\n",
      "\n",
      "[2023-09-27 13:32:26] Features: 5/40 -- score: 0.8249425009812066[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   23.6s\n",
      "\n",
      "[2023-09-27 13:32:56] Features: 6/40 -- score: 0.8134592932189699[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   25.7s\n",
      "\n",
      "[2023-09-27 13:33:27] Features: 7/40 -- score: 0.8008166302313192[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   26.4s\n",
      "\n",
      "[2023-09-27 13:33:59] Features: 8/40 -- score: 0.7906782725120882[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   28.5s\n",
      "\n",
      "[2023-09-27 13:34:33] Features: 9/40 -- score: 0.7738976002771825[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   39.9s\n",
      "\n",
      "[2023-09-27 13:35:19] Features: 10/40 -- score: 0.7311267178202511[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   43.4s\n",
      "\n",
      "[2023-09-27 13:36:07] Features: 11/40 -- score: 0.6755770943771549[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   45.2s\n",
      "\n",
      "[2023-09-27 13:36:57] Features: 12/40 -- score: 0.6512266762588402[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   49.5s\n",
      "\n",
      "[2023-09-27 13:37:50] Features: 13/40 -- score: 0.6269260714806116[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.1min\n",
      "\n",
      "[2023-09-27 13:38:59] Features: 14/40 -- score: 0.6289647091866234[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.3min\n",
      "\n",
      "[2023-09-27 13:40:18] Features: 15/40 -- score: 0.6375354052947466[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:  1.5min\n",
      "\n",
      "[2023-09-27 13:41:47] Features: 16/40 -- score: 0.6436388199368147\n",
      "[2023-09-27 13:43:40] Features: 17/40 -- score: 0.6468364512809603\n",
      "[2023-09-27 13:45:28] Features: 18/40 -- score: 0.6479099889598336\n",
      "[2023-09-27 13:47:16] Features: 19/40 -- score: 0.6502101605638277\n",
      "[2023-09-27 13:49:13] Features: 20/40 -- score: 0.6493381365262852\n",
      "[2023-09-27 13:51:18] Features: 21/40 -- score: 0.6478644131748377\n",
      "[2023-09-27 13:53:36] Features: 22/40 -- score: 0.6497695833968326\n",
      "[2023-09-27 13:55:56] Features: 23/40 -- score: 0.6465983592533802\n",
      "[2023-09-27 13:58:29] Features: 24/40 -- score: 0.6466300224618585\n",
      "[2023-09-27 14:01:06] Features: 25/40 -- score: 0.646092660162532\n",
      "[2023-09-27 14:03:51] Features: 26/40 -- score: 0.6438039799703491\n",
      "[2023-09-27 14:06:33] Features: 27/40 -- score: 0.6433365619201852\n",
      "[2023-09-27 14:09:16] Features: 28/40 -- score: 0.6441899341220994\n",
      "[2023-09-27 14:12:02] Features: 29/40 -- score: 0.6419276058995788\n",
      "[2023-09-27 14:14:44] Features: 30/40 -- score: 0.6430703283296991\n",
      "[2023-09-27 14:17:29] Features: 31/40 -- score: 0.6430936720427523\n",
      "[2023-09-27 14:20:12] Features: 32/40 -- score: 0.6435781169114746\n",
      "[2023-09-27 14:22:56] Features: 33/40 -- score: 0.6447534754774529\n",
      "[2023-09-27 14:26:04] Features: 34/40 -- score: 0.6430316560722898\n",
      "[2023-09-27 14:29:07] Features: 35/40 -- score: 0.6417001534528338\n",
      "[2023-09-27 14:32:05] Features: 36/40 -- score: 0.6389970678685502\n",
      "[2023-09-27 14:35:05] Features: 37/40 -- score: 0.6400327919380353\n",
      "[2023-09-27 14:37:59] Features: 38/40 -- score: 0.6427654346430639\n",
      "[2023-09-27 14:40:48] Features: 39/40 -- score: 0.6437121226423619\n",
      "[2023-09-27 14:43:33] Features: 40/40 -- score: 0.6467725665724771"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "fwd_feature_selector = SFS(DecisionTreeClassifier(), k_features=(10,40), forward=True,\n",
    "                                                  floating=False, verbose=2, scoring='roc_auc', cv=5).fit(X_train, y_train)\n",
    "#cv is croos validation 5 times\n",
    "#The cv=5 parameter means that this process is done using 5-fold cross-validation on your training data (X_train and y_train). \n",
    "# In each fold of the cross-validation, 4/5 of the data is used for training and 1/5 is used for validation.\n",
    "# The ROC AUC score is averaged over the 5 folds to give a more robust estimate of the model’s performance.\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_feature_selector.k_feature_names_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd_feature_selector.k_feature_names_     # to get the final set of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "SequentialFeatureSelector has not been fitted, yet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\hamza\\Documents\\7th Sem\\IDM\\Challenge 1\\FwdselctionDataCleaning.ipynb Cell 32\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Transform X_test to contain only the selected features\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m X_test_transformed \u001b[39m=\u001b[39m fwd_feature_selector\u001b[39m.\u001b[39;49mtransform(X_test)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# Fit the model on the transformed X_train\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# model = DecisionTreeClassifier(max_depth=2, min_samples_split= 10)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/hamza/Documents/7th%20Sem/IDM/Challenge%201/FwdselctionDataCleaning.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m model \u001b[39m=\u001b[39m DecisionTreeClassifier()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mlxtend\\feature_selection\\sequential_feature_selector.py:823\u001b[0m, in \u001b[0;36mSequentialFeatureSelector.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtransform\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Reduce X to its most important features.\u001b[39;00m\n\u001b[0;32m    809\u001b[0m \n\u001b[0;32m    810\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    821\u001b[0m \n\u001b[0;32m    822\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 823\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_fitted()\n\u001b[0;32m    824\u001b[0m     X_, _ \u001b[39m=\u001b[39m _preprocess(X)\n\u001b[0;32m    825\u001b[0m     \u001b[39mreturn\u001b[39;00m X_[:, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_feature_idx_]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\mlxtend\\feature_selection\\sequential_feature_selector.py:898\u001b[0m, in \u001b[0;36mSequentialFeatureSelector._check_fitted\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    896\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_fitted\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    897\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfitted:\n\u001b[1;32m--> 898\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    899\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mSequentialFeatureSelector has not been\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m fitted, yet.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    900\u001b[0m         )\n",
      "\u001b[1;31mAttributeError\u001b[0m: SequentialFeatureSelector has not been fitted, yet."
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Transform X_test to contain only the selected features\n",
    "X_test_transformed = fwd_feature_selector.transform(X_test)\n",
    "\n",
    "# Fit the model on the transformed X_train\n",
    "# model = DecisionTreeClassifier(max_depth=2, min_samples_split= 10)\n",
    "model = DecisionTreeClassifier()\n",
    "model.fit(fwd_feature_selector.transform(X_train), y_train)\n",
    "\n",
    "\n",
    "# Predict and evaluate on the transformed X_test\n",
    "y_pred = model.predict_proba(X_test_transformed)[:, 1]\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_transformedTest = fwd_feature_selector.transform(Test)\n",
    "X_test_transformedTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict and evaluate on the original Test set\n",
    "hospital_death_prob = model.predict_proba(Test)\n",
    "# The predicted probabilities of each class are stored in the second column of the output array\n",
    "hospital_death = hospital_death_prob[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionsFWDSLEECTION.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RecordID</th>\n",
       "      <th>hospital_death</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50001.0</td>\n",
       "      <td>0.008349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50002.0</td>\n",
       "      <td>0.815385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50003.0</td>\n",
       "      <td>0.092235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>50004.0</td>\n",
       "      <td>0.022912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50005.0</td>\n",
       "      <td>0.072663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RecordID  hospital_death\n",
       "0   50001.0        0.008349\n",
       "1   50002.0        0.815385\n",
       "2   50003.0        0.092235\n",
       "3   50004.0        0.022912\n",
       "4   50005.0        0.072663"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    4.3s\n",
      "\n",
      "[2023-09-27 20:10:46] Features: 1/40 -- score: 0.8341686273881223[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.8s\n",
      "\n",
      "[2023-09-27 20:10:56] Features: 2/40 -- score: 0.8327600432675659[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.3s\n",
      "\n",
      "[2023-09-27 20:11:06] Features: 3/40 -- score: 0.8328634470830423[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.9s\n",
      "\n",
      "[2023-09-27 20:11:17] Features: 4/40 -- score: 0.8323681088118089[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.5s\n",
      "\n",
      "[2023-09-27 20:11:26] Features: 5/40 -- score: 0.8320388674565702[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    6.9s\n",
      "\n",
      "[2023-09-27 20:11:35] Features: 6/40 -- score: 0.833900450692826[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.5s\n",
      "\n",
      "[2023-09-27 20:11:44] Features: 7/40 -- score: 0.8383712838439411[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    7.5s\n",
      "\n",
      "[2023-09-27 20:11:53] Features: 8/40 -- score: 0.8424352454767741[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    8.1s\n",
      "\n",
      "[2023-09-27 20:12:03] Features: 9/40 -- score: 0.8445744955796401[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    8.8s\n",
      "\n",
      "[2023-09-27 20:12:13] Features: 10/40 -- score: 0.8468260219500751[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    8.9s\n",
      "\n",
      "[2023-09-27 20:12:23] Features: 11/40 -- score: 0.8478796379958705[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    9.4s\n",
      "\n",
      "[2023-09-27 20:12:34] Features: 12/40 -- score: 0.8489726347715415[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:    9.9s\n",
      "\n",
      "[2023-09-27 20:12:45] Features: 13/40 -- score: 0.8500182918694458[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   10.1s\n",
      "\n",
      "[2023-09-27 20:12:55] Features: 14/40 -- score: 0.8506734799467714[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   10.4s\n",
      "\n",
      "[2023-09-27 20:13:06] Features: 15/40 -- score: 0.8512227691267817[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   10.6s\n",
      "[Parallel(n_jobs=1)]: Done  40 tasks      | elapsed:   10.6s\n",
      "\n",
      "[2023-09-27 20:13:17] Features: 16/40 -- score: 0.8514695039562467\n",
      "[2023-09-27 20:13:28] Features: 17/40 -- score: 0.8515561349509113\n",
      "[2023-09-27 20:13:39] Features: 18/40 -- score: 0.8515170733346885\n",
      "[2023-09-27 20:13:50] Features: 19/40 -- score: 0.8514657823674249\n",
      "[2023-09-27 20:14:00] Features: 20/40 -- score: 0.8517501885543493\n",
      "[2023-09-27 20:14:11] Features: 21/40 -- score: 0.8517258116916864\n",
      "[2023-09-27 20:14:22] Features: 22/40 -- score: 0.8516640494655494\n",
      "[2023-09-27 20:14:32] Features: 23/40 -- score: 0.8515220144227248\n",
      "[2023-09-27 20:14:43] Features: 24/40 -- score: 0.8512937450530604\n",
      "[2023-09-27 20:14:54] Features: 25/40 -- score: 0.8508965200343838\n",
      "[2023-09-27 20:15:04] Features: 26/40 -- score: 0.8508676392424415\n",
      "[2023-09-27 20:15:15] Features: 27/40 -- score: 0.8515565059645371\n",
      "[2023-09-27 20:15:25] Features: 28/40 -- score: 0.8517588587630076\n",
      "[2023-09-27 20:15:36] Features: 29/40 -- score: 0.8513404881766077\n",
      "[2023-09-27 20:15:47] Features: 30/40 -- score: 0.851025354847093\n",
      "[2023-09-27 20:15:57] Features: 31/40 -- score: 0.8504264007875413\n",
      "[2023-09-27 20:16:07] Features: 32/40 -- score: 0.8497543936495481\n",
      "[2023-09-27 20:16:17] Features: 33/40 -- score: 0.8490507619530652\n",
      "[2023-09-27 20:16:27] Features: 34/40 -- score: 0.8483148253944256\n",
      "[2023-09-27 20:16:36] Features: 35/40 -- score: 0.8476727114211599\n",
      "[2023-09-27 20:16:45] Features: 36/40 -- score: 0.8470758072588307\n",
      "[2023-09-27 20:16:54] Features: 37/40 -- score: 0.8469957142082943\n",
      "[2023-09-27 20:17:03] Features: 38/40 -- score: 0.8466207930813543\n",
      "[2023-09-27 20:17:11] Features: 39/40 -- score: 0.8460324435722827\n",
      "[2023-09-27 20:17:20] Features: 40/40 -- score: 0.8455098929168047"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "fwd_feature_selectorNB = SFS(GaussianNB(), k_features=(10,40), forward=True,\n",
    "                                                  floating=False, verbose=2, scoring='roc_auc', cv=5).fit(X_train, y_train)\n",
    "#cv is croos validation 5 times\n",
    "#The cv=5 parameter means that this process is done using 5-fold cross-validation on your training data (X_train and y_train). \n",
    "# In each fold of the cross-validation, 4/5 of the data is used for training and 1/5 is used for validation.\n",
    "# The ROC AUC score is averaged over the 5 folds to give a more robust estimate of the model’s performance.\n",
    "                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hospital_id',\n",
       " 'icu_id',\n",
       " 'ethnicity',\n",
       " 'icu_admit_source',\n",
       " 'icu_type',\n",
       " 'apache_2_bodysystem',\n",
       " 'age',\n",
       " 'pre_icu_los_days',\n",
       " 'apache_3j_diagnosis',\n",
       " 'gcs_verbal_apache',\n",
       " 'heart_rate_apache',\n",
       " 'resprate_apache',\n",
       " 'ventilated_apache',\n",
       " 'd1_diasbp_min',\n",
       " 'd1_heartrate_max',\n",
       " 'd1_resprate_max',\n",
       " 'd1_spo2_min',\n",
       " 'd1_sysbp_min',\n",
       " 'd1_temp_min',\n",
       " 'h1_mbp_max',\n",
       " 'h1_mbp_noninvasive_max',\n",
       " 'h1_resprate_min',\n",
       " 'h1_spo2_max',\n",
       " 'h1_spo2_min',\n",
       " 'h1_sysbp_max',\n",
       " 'd1_potassium_max',\n",
       " 'apache_4a_hospital_death_prob',\n",
       " 'apache_4a_icu_death_prob')"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fwd_feature_selectorNB.k_feature_names_     # to get the final set of features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000,)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Transform X_test to contain only the selected features\n",
    "X_test_transformed = fwd_feature_selectorNB.transform(X_test)\n",
    "\n",
    "# Fit the model on the transformed X_train\n",
    "model = GaussianNB()\n",
    "model.fit(fwd_feature_selectorNB.transform(X_train), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999859</td>\n",
       "      <td>0.000141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999800</td>\n",
       "      <td>0.000200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999971</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.998427</td>\n",
       "      <td>0.001573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998547</td>\n",
       "      <td>0.001453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14995</th>\n",
       "      <td>0.999953</td>\n",
       "      <td>0.000047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14996</th>\n",
       "      <td>0.999957</td>\n",
       "      <td>0.000043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14997</th>\n",
       "      <td>0.999951</td>\n",
       "      <td>0.000049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14998</th>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.000308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14999</th>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.000020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1\n",
       "0      0.999859  0.000141\n",
       "1      0.999800  0.000200\n",
       "2      0.999971  0.000029\n",
       "3      0.998427  0.001573\n",
       "4      0.998547  0.001453\n",
       "...         ...       ...\n",
       "14995  0.999953  0.000047\n",
       "14996  0.999957  0.000043\n",
       "14997  0.999951  0.000049\n",
       "14998  0.999692  0.000308\n",
       "14999  0.999980  0.000020\n",
       "\n",
       "[15000 rows x 2 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert y_pred to a pandas DataFrame\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "\n",
    "# Print the columns of y_pred_df\n",
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441829535322867\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Transform X_test to contain only the selected features\n",
    "X_test_transformed = fwd_feature_selectorNB.transform(X_test)\n",
    "\n",
    "# Fit the model on the transformed X_train\n",
    "model = GaussianNB(var_smoothing=1e-9)\n",
    "model.fit(fwd_feature_selectorNB.transform(X_train), y_train)\n",
    "\n",
    "# Predict and evaluate on the transformed X_test\n",
    "y_pred = model.predict_proba(X_test_transformed)[:, 1]\n",
    "print(roc_auc_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Transform X_test to contain only the selected features\n",
    "X_test_transformedTEST = fwd_feature_selectorNB.transform(Test)\n",
    "# Predict and evaluate on the transformed X_test\n",
    "hospital_death_prob = model.predict_proba(X_test_transformedTEST)[:, 1]\n",
    " \n",
    "# Create a DataFrame for the predictions\n",
    "predictions_df = pd.DataFrame(hospital_death_prob, columns=['hospital_death'])\n",
    "\n",
    "# Add the record ID from the test data to the predictions DataFrame\n",
    "predictions_df.insert(0, 'RecordID', Test['RecordID'])\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "predictions_df.to_csv('predictionsFWDSLEECTION.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 2)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
